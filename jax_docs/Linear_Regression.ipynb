{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with JAX\n",
    "### Using Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://coax.readthedocs.io/en/latest/examples/linear_regression/jax.html     \n",
    "https://www.youtube.com/watch?v=aOsZdf9tiNQ    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp # JAX's numpy module\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (100, 3)\n",
      "y.shape: (100,)\n",
      "\n",
      "X_test.shape: (25, 3)\n",
      "y_test.shape: (25,)\n",
      "\n",
      "X.shape: (75, 3)\n",
      "y.shape: (75,)\n",
      "\n",
      "X[:5]:\n",
      "[[-0.63314246  1.72607764  0.99368579]\n",
      " [ 1.26313396  0.45627519  2.04240246]\n",
      " [-1.81818504 -0.18394181 -1.12144175]\n",
      " [-0.76760346  0.21830801 -2.06919072]\n",
      " [-0.47773342  0.23747435  0.74183241]]\n",
      "\n",
      "y[:5]:\n",
      " [  94.91726361  229.47039647 -191.33008049 -175.01757296   29.57724779]\n"
     ]
    }
   ],
   "source": [
    "# The dataset\n",
    "X, y = make_regression(n_features=3)\n",
    "print(f\"X.shape: {X.shape}\") # 100 data points of 3 features.\n",
    "print(f\"y.shape: {y.shape}\") # a float number for each data point.\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "print(f\"\\nX_test.shape: {X_test.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")\n",
    "print(f\"\\nX.shape: {X.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"\\nX[:5]:\\n{X[:5]}\") # X first 5 rows\n",
    "print(f\"\\ny[:5]:\\n {y[:5]}\") # y first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': Array([0., 0., 0.], dtype=float32), 'b': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model parameters, weights and bias\n",
    "params = {\n",
    "    'w': jnp.zeros(X.shape[1:]), # the shape of the input without the batch dimension, initialized to zeros.\n",
    "    'b': 0. # a float, the y intercept.Initialized to zero.\n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "\n",
      "forward pass shape: (75,)\n"
     ]
    }
   ],
   "source": [
    "def forward(params, X):\n",
    "    return jnp.dot(X, params['w']) + params['b']\n",
    "\n",
    "# Sanity\n",
    "print(forward(params, X))\n",
    "print(f\"\\nforward pass shape: {forward(params, X).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function - MSE (Mean Squared Error)\n",
    "MSE stands for Mean Squared Error. It is a common loss function used in regression models to measure the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "Here's a brief explanation of how MSE is calculated:\n",
    "\n",
    "Calculate the error: Subtract the actual value from the predicted value for each data point.\n",
    "Square the error: Square each of these errors to ensure they are positive and to penalize larger errors more.\n",
    "Mean of squared errors: Calculate the mean (average) of these squared errors.\n",
    "The formula for MSE is:\n",
    "\n",
    "[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 ]\n",
    "\n",
    "Where:\n",
    "\n",
    "( n ) is the number of data points.\n",
    "( y_i ) is the actual value.\n",
    "( \\hat{y}_i ) is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11719.045\n",
      "16463.441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(11719.045, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit # Just-in-time compilation decorator, increase performance\n",
    "def loss_fn(params, X, y):\n",
    "    err = forward(params, X) - y # the error/residual is the prediction (forward pass return) - ground truth (y)\n",
    "    return jnp.mean(jnp.square(err))  # return the MSE (Mean Squared Error)\n",
    "\n",
    "# Sanity\n",
    "print(loss_fn(params, X, y))\n",
    "print(loss_fn(params, X_test, y_test))\n",
    "loss_fn(params, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': Array(14.363653, dtype=float32, weak_type=True),\n",
       " 'w': Array([-110.79656, -105.89562, -190.3433 ], dtype=float32)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JAX calculates the gradient for us\n",
    "# Takes the loss function, the parameters, and the input data as arguments\n",
    "# Returns the derivative with respect to params\n",
    "# Returns the same structure as the parameters\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "# Sanity\n",
    "grad_fn(params, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, grads):\n",
    "    \"\"\"\n",
    "    Update the parameters by taking a small step in the negative direction of teh gradients.\n",
    "    \"\"\"\n",
    "    # return jax.tree_map(lambda p, g: p - LEARNING_RATE * g, params, grads)\n",
    "    return jax.tree.map(lambda p, g: p - LEARNING_RATE * g, params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25933.582\n",
      "20410.049\n",
      "16064.913\n",
      "12646.394\n",
      "9956.582\n",
      "7839.903\n",
      "6174.0356\n",
      "4862.8076\n",
      "3830.5942\n",
      "3017.922\n",
      "2378.0134\n",
      "1874.0739\n",
      "1477.1584\n",
      "1164.4943\n",
      "918.1621\n",
      "724.06116\n",
      "571.0939\n",
      "450.5239\n",
      "355.47495\n",
      "280.5327\n",
      "221.43373\n",
      "174.82013\n",
      "138.04834\n",
      "109.034424\n",
      "86.13783\n",
      "68.06493\n",
      "53.796814\n",
      "42.53005\n",
      "33.631374\n",
      "26.60159\n",
      "21.046825\n",
      "16.656607\n",
      "13.185933\n",
      "10.44149\n",
      "8.270813\n",
      "6.553452\n",
      "5.194402\n",
      "4.118541\n",
      "3.2666554\n",
      "2.591905\n",
      "2.057276\n",
      "1.6335433\n",
      "1.2975923\n",
      "1.0311568\n",
      "0.8197599\n",
      "0.65198505\n",
      "0.5187718\n",
      "0.41296473\n",
      "0.32888302\n",
      "0.26204428\n",
      "0.20888999\n",
      "0.16660687\n",
      "0.1329463\n",
      "0.10613999\n",
      "0.08478629\n",
      "0.06776581\n",
      "0.05419131\n",
      "0.04335986\n",
      "0.03471623\n",
      "0.027810927\n",
      "0.022293288\n",
      "0.017880581\n",
      "0.014349177\n",
      "0.011523679\n",
      "0.009259203\n",
      "0.007445076\n",
      "0.005990444\n",
      "0.00482298\n",
      "0.0038861656\n",
      "0.0031330797\n",
      "0.0025274695\n",
      "0.0020402547\n",
      "0.001648122\n",
      "0.0013322071\n",
      "0.0010774925\n",
      "0.00087196496\n"
     ]
    }
   ],
   "source": [
    "# the main training loop\n",
    "for _ in range(EPOCHS):\n",
    "    loss = loss_fn(params, X_test, y_test)\n",
    "    print(loss)\n",
    "    if loss < 0.001: # just for me, for output clarity\n",
    "        break\n",
    "\n",
    "    grads = grad_fn(params, X, y) # Don't calculate grads on the test set (that would be cheating)\n",
    "    params = update(params, grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
