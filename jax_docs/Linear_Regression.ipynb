{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with JAX\n",
    "### Using Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://coax.readthedocs.io/en/latest/examples/linear_regression/jax.html     \n",
    "https://www.youtube.com/watch?v=aOsZdf9tiNQ    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp # JAX's numpy module\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 # 500\n",
    "LEARNING_RATE = 0.1 # 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (100, 3)\n",
      "y.shape: (100,)\n",
      "\n",
      "X_test.shape: (25, 3)\n",
      "y_test.shape: (25,)\n",
      "\n",
      "X.shape: (75, 3)\n",
      "y.shape: (75,)\n",
      "\n",
      "X[:5]:\n",
      "[[-0.08558383 -0.09131494  0.92575657]\n",
      " [-0.04107166 -0.88512454  1.54184891]\n",
      " [ 0.41668555  0.46752467  0.23619388]\n",
      " [-0.20825448  0.44464659 -0.53490139]\n",
      " [ 0.19027261  0.42155077 -1.20302263]]\n",
      "\n",
      "y[:5]:\n",
      " [ 12.33603052 -17.5296469   45.65653555   6.39202706   3.76626716]\n"
     ]
    }
   ],
   "source": [
    "# The dataset\n",
    "X, y = make_regression(n_features=3)\n",
    "print(f\"X.shape: {X.shape}\") # 100 data points of 3 features.\n",
    "print(f\"y.shape: {y.shape}\") # a float number for each data point.\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "print(f\"\\nX_test.shape: {X_test.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")\n",
    "print(f\"\\nX.shape: {X.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"\\nX[:5]:\\n{X[:5]}\") # X first 5 rows\n",
    "print(f\"\\ny[:5]:\\n {y[:5]}\") # y first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': Array([0., 0., 0.], dtype=float32), 'b': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model parameters, weights and bias\n",
    "params = {\n",
    "    'w': jnp.zeros(X.shape[1:]), # the shape of the input without the batch dimension, initialized to zeros.\n",
    "    'b': 0. # a float, the y intercept.Initialized to zero.\n",
    "}\n",
    "params\n",
    "\n",
    "# model parameters, weights and bias\n",
    "# params = {\n",
    "#     'w': jax.random.uniform(key=jax.random.PRNGKey(0), shape=X.shape[1:]), # the shape of the input without the batch dimension, initialized to zeros.\n",
    "#     'b': 0. # a float, the y intercept.Initialized to zero.\n",
    "# }\n",
    "# params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "\n",
      "forward pass shape: (75,)\n"
     ]
    }
   ],
   "source": [
    "def forward(params, X):\n",
    "    # return jnp.dot(X, params['w'].T) + params['b'] # Transpose the weights\n",
    "    return jnp.dot(X, params['w']) + params['b']\n",
    "\n",
    "# Sanity\n",
    "print(forward(params, X))\n",
    "print(f\"\\nforward pass shape: {forward(params, X).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function - MSE (Mean Squared Error)\n",
    "MSE stands for Mean Squared Error. It is a common loss function used in regression models to measure the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "Here's a brief explanation of how MSE is calculated:\n",
    "\n",
    "Calculate the error: Subtract the actual value from the predicted value for each data point.\n",
    "Square the error: Square each of these errors to ensure they are positive and to penalize larger errors more.\n",
    "Mean of squared errors: Calculate the mean (average) of these squared errors.\n",
    "The formula for MSE is:\n",
    "\n",
    "[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 ]\n",
    "\n",
    "Where:\n",
    "\n",
    "( n ) is the number of data points.\n",
    "( y_i ) is the actual value.\n",
    "( \\hat{y}_i ) is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755.4663\n",
      "3348.2605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(4755.4663, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit # Just-in-time compilation decorator, increase performance\n",
    "def loss_fn(params, X, y):\n",
    "    err = forward(params, X) - y # the error/residual is the prediction (forward pass return) - ground truth (y)\n",
    "    return jnp.mean(jnp.square(err))  # return the MSE (Mean Squared Error)\n",
    "\n",
    "# Sanity\n",
    "print(loss_fn(params, X, y))\n",
    "print(loss_fn(params, X_test, y_test))\n",
    "loss_fn(params, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': Array(12.478823, dtype=float32, weak_type=True),\n",
       " 'w': Array([ -64.38199 , -119.19191 ,  -27.376173], dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JAX calculates the gradient for us\n",
    "# Takes the loss function, the parameters, and the input data as arguments\n",
    "# Returns the derivative with respect to params\n",
    "# Returns the same structure as the parameters\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "# Sanity\n",
    "grad_fn(params, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': Array(-1.2478822, dtype=float32, weak_type=True),\n",
       " 'w': Array([ 6.438199 , 11.919191 ,  2.7376173], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update(params, grads):\n",
    "    \"\"\"\n",
    "    Update the parameters by taking a small step in the negative direction of the gradients.\n",
    "    Uses a JAX utility of pytrees:\n",
    "    https://jax.readthedocs.io/en/latest/pytrees.html\n",
    "    The lambda function acts on the leaves of those pytrees.\n",
    "    In this case it acts only on the values of the dictionaries and not the keys.\n",
    "    For each leaf parameter we want to subtract the learning rate times (in the direction of) the gradient.\n",
    "\n",
    "    \"\"\"\n",
    "    return jax.tree.map(lambda p, g: p - LEARNING_RATE * g, params, grads)\n",
    "\n",
    "# Sanity\n",
    "update(params, grad_fn(params, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss: 3348.260498046875\n",
      "\n",
      "loss: 2097.861572265625\n",
      "\n",
      "loss: 1320.1396484375\n",
      "\n",
      "loss: 834.7333374023438\n",
      "\n",
      "loss: 530.6007080078125\n",
      "\n",
      "loss: 339.22821044921875\n",
      "\n",
      "loss: 218.23890686035156\n",
      "\n",
      "loss: 141.3498077392578\n",
      "\n",
      "loss: 92.20957946777344\n",
      "\n",
      "loss: 60.61093521118164\n",
      "\n",
      "loss: 40.15789794921875\n",
      "\n",
      "loss: 26.825946807861328\n",
      "\n",
      "loss: 18.07109260559082\n",
      "\n",
      "loss: 12.277174949645996\n",
      "\n",
      "loss: 8.411867141723633\n",
      "\n",
      "loss: 5.8119120597839355\n",
      "\n",
      "loss: 4.048447132110596\n",
      "\n",
      "loss: 2.8423237800598145\n",
      "\n",
      "loss: 2.010556221008301\n",
      "\n",
      "loss: 1.432295322418213\n",
      "\n",
      "loss: 1.0271083116531372\n",
      "\n",
      "loss: 0.741074800491333\n",
      "\n",
      "loss: 0.5377066731452942\n",
      "\n",
      "loss: 0.3921509385108948\n",
      "\n",
      "loss: 0.2873229384422302\n",
      "\n",
      "loss: 0.21139657497406006\n",
      "\n",
      "loss: 0.15611524879932404\n",
      "\n",
      "loss: 0.11567297577857971\n",
      "\n",
      "loss: 0.08595991134643555\n",
      "\n",
      "loss: 0.06404363363981247\n",
      "\n",
      "loss: 0.04782363772392273\n",
      "\n",
      "loss: 0.03578297793865204\n",
      "\n",
      "loss: 0.026820559054613113\n",
      "\n",
      "loss: 0.020133327692747116\n",
      "\n",
      "loss: 0.01513282023370266\n",
      "\n",
      "loss: 0.011386659927666187\n",
      "\n",
      "loss: 0.008576474152505398\n",
      "\n",
      "loss: 0.006465078331530094\n",
      "\n",
      "loss: 0.004876935388892889\n",
      "\n",
      "loss: 0.0036811642348766327\n",
      "\n",
      "loss: 0.0027798276860266924\n",
      "\n",
      "loss: 0.0021000795532017946\n",
      "\n",
      "loss: 0.0015871021896600723\n",
      "\n",
      "loss: 0.0011997116962447762\n",
      "\n",
      "loss: 0.0009070358937606215\n",
      "\n",
      "loss: 0.0006859909626655281\n",
      "\n",
      "loss: 0.0005188772920519114\n",
      "\n",
      "loss: 0.00039242411730811\n",
      "\n",
      "loss: 0.0002967856707982719\n",
      "\n",
      "loss: 0.00022452647681348026\n"
     ]
    }
   ],
   "source": [
    "# the main training loop\n",
    "for _ in range(EPOCHS):\n",
    "    # print(f\"\\nparams:\\n{forward(params, X)}\")\n",
    "    loss = loss_fn(params, X_test, y_test)\n",
    "    print(f\"\\nloss: {loss}\")\n",
    "    if loss < 0.001: # just for me, for output clarity\n",
    "        break\n",
    "\n",
    "    grads = grad_fn(params, X, y) # Don't calculate grads on the test set (that would be cheating)\n",
    "\n",
    "    \"\"\"\n",
    "    The following two lines are not optimal.\n",
    "    Especially if we have many parameters, because then we will have to do this \n",
    "    for every parameter in the dictionary.\n",
    "    Instead I use the 'update' function that uses JAX utility.\n",
    "    \"\"\"\n",
    "    # params['w'] -= LEARNING_RATE * grads['w']\n",
    "    # params['b'] -= LEARNING_RATE * grads['b']\n",
    "\n",
    "    params = update(params, grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
