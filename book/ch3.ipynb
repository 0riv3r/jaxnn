{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.30\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is gradient:   \n",
    "https://www.youtube.com/watch?v=6zgBUZuC-p8&list=PLg5nrpKdkk2DpW_a-kuHU_FsVPPaU447J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x=2.0: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Automatic Differentiation\n",
    "# Location 329\n",
    "# First code example\n",
    "import jax\n",
    "\n",
    "\"\"\"\n",
    "Jax's 'grad' function calculates the gradient of a function.\n",
    "The function takes two arguments:\n",
    "- the target function to calculate the gradient of\n",
    "- the index of the argument to calculate the gradient with respect to\n",
    "\n",
    "The result of this function ('jax.grad') is a new function ('simple_function_grad') that calculates the gradient of the target function with respect to the specified argument.\n",
    "The result of this new function is the derivative of the target function at the specified point.\n",
    "\"\"\"\n",
    "\n",
    "# define the target function to verify its gradient\n",
    "def simple_function(x):\n",
    "  return 2*x + 9\n",
    "  # return jax.numpy.sin(x)\n",
    "\n",
    "# calculate the gradient of the target function\n",
    "simple_function_grad = jax.grad(simple_function)\n",
    "\n",
    "# Evaluate the gradient at a specific point - the derivative of the function at that point\n",
    "result = simple_function_grad(2.0)  # 3.0 is the point at which the gradient is evaluated\n",
    "print(f\"Gradient at x=2.0: {result}\") # The derivative of the function at the specified point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized matrix multiplication: [[19 22]\n",
      " [43 50]]\n",
      "XLA optimized matrix multiplication: [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Matrix Multiplication using XLA\n",
    "\"\"\"\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "def mathmul(A, B):\n",
    "  return (A @ B)\n",
    "\n",
    "@jax.jit\n",
    "def mathmul_jit(A, B): \n",
    "  return (A @ B)\n",
    "\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Unoptimized matrix multiplication\n",
    "C = mathmul(A, B)\n",
    "print(f\"Unoptimized matrix multiplication: {C}\")\n",
    "\n",
    "# XLA optimized matrix multiplication\n",
    "D = mathmul_jit(A, B)\n",
    "print(f\"XLA optimized matrix multiplication: {D}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timing and comparing functions\n",
    "\"\"\"\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "def compare(func, n, A, M):\n",
    "\n",
    "  def time_func(func, n, A, M):\n",
    "    for i in range(n):\n",
    "      M = func(A, M)\n",
    "      #print(f\"M: {M}\")\n",
    "    return(func)\n",
    "\n",
    "  # def time_func(func, n, A, M):\n",
    "  #   if n > 0:\n",
    "  #     new_M = func(A, M)\n",
    "  #     time_func(func, n-1, A, new_M)\n",
    "  #   return(func)\n",
    "\n",
    "  t1_start = perf_counter()\n",
    "  used_func = time_func(func, n, A, B)\n",
    "  print(f\"used_func: {used_func}\")\n",
    "  # Stop the stopwatch / counter\n",
    "  t1_stop = perf_counter()\n",
    "\n",
    "  return (t1_stop-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used_func: <function mathmul at 0x00000203866D5580>\n",
      "used_func: <PjitFunction of <function mathmul_jit at 0x00000203866D5620>>\n",
      "unoptimized_function_time: 4.521084500000143 \n",
      "optimized_function_time: 3.287428900002851 \n",
      "Optimization value: 1.2336555999972916\n"
     ]
    }
   ],
   "source": [
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "n = 1000000\n",
    "\n",
    "unoptimized_function_time = compare(mathmul, n, A, B)\n",
    "optimized_function_time = compare(mathmul_jit, n, A, B)\n",
    "\n",
    "print(f\"unoptimized_function_time: {unoptimized_function_time} \")\n",
    "print(f\"optimized_function_time: {optimized_function_time} \")\n",
    "\n",
    "assert(optimized_function_time < unoptimized_function_time)\n",
    "print(f\"Optimization value: {unoptimized_function_time - optimized_function_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eye (one-hot vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "one-hot vector\n",
    "Each number in the target vector is converted into 1 that is placed in its value placement, \n",
    "while all the rest of the row are zeros.\n",
    "np.eye(max-value+1)[vector]\n",
    "\n",
    "\"\"\"\n",
    "v = np.array([1, 4, 2, 1, 0, 1, 3, 2])\n",
    "np.eye(np.max(v)+1)[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot without eye\n",
    "\n",
    "https://wandb.ai/mostafaibrahim17/ml-articles/reports/One-Hot-Encoding-Creating-a-NumPy-Array-Using-Weights-Biases--Vmlldzo2MzQzNTQ5#:~:text=Array%20in%20NumPy-,To%20generate%20one-hot%20encodings%20for%20an%20array%20in%20NumPy,to%20its%20category%20to%201.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1, 4, 2, 1, 0, 1, 3, 2])\n",
    "m = np.zeros((v.size, v.max() + 1))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(v.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[np.arange(v.size), v] = 1\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m[np.arange(v.size), v] = 1      \n",
    "m[ [0, 1, 2, 3, 4, 5, 6, 7], [1, 4, 2, 1, 0, 1, 3, 2] ] = 1      \n",
    "m[0,1] = 1      \n",
    "m[1,4] = 1      \n",
    "m[2,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CH3 Coding Challenge  \n",
    "Matrix Power and XLA optimization    \n",
    "(location 419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_result_unoptimized: [[2441407. 7324218.]\n",
      " [2441406. 7324219.]]\n",
      "power_result_optimized: [[2441407. 7324218.]\n",
      " [2441406. 7324219.]]\n",
      "Unoptimized matrix power time: 0.08302760000060516\n",
      "Optimized matrix power time: 0.03413060000002588\n",
      "Optimization value: 0.04889700000057928\n",
      "power_result_optimized_2: [[2441407. 7324218.]\n",
      " [2441406. 7324219.]]\n",
      "Optimized matrix power time: 5.2800000048591755e-05\n",
      "Optimization second time: 0.08297480000055657\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from time import perf_counter\n",
    "\n",
    "matrix = jnp.array([[2,3], [1,4]])\n",
    "power = 10\n",
    "\n",
    "# Calculate matrix power\n",
    "def matrix_power(_matrix, _power):\n",
    "  result = jnp.eye(_matrix.shape[0])\n",
    "  for _ in range(_power):\n",
    "    result = result @ _matrix\n",
    "  return result\n",
    "\n",
    "# Unoptimized matrix power\n",
    "t1_start = perf_counter()\n",
    "power_result_unoptimized = matrix_power(matrix, power)\n",
    "t1_stop = perf_counter()\n",
    "\n",
    "print(f\"power_result_unoptimized: {power_result_unoptimized}\")\n",
    "\n",
    "\"\"\"\n",
    "XLA optimized matrix power using @jax.jit\n",
    "\n",
    "Due to the following:\n",
    "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
    "https://jax.readthedocs.io/en/latest/faq.html#faq-different-kinds-of-jax-values\n",
    "https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError\n",
    "https://jax.readthedocs.io/en/latest/faq.html#how-can-i-convert-a-jax-tracer-to-a-numpy-array\n",
    "\n",
    "If I do the following:\n",
    "@jax.jit\n",
    "def matrix_power_jit(matrix, power):\n",
    "  result = jnp.eye(matrix.shape[0])\n",
    "  for _ in range(power):\n",
    "    result = result @ matrix\n",
    "  return result\n",
    "\n",
    "power_result_optimized = matrix_power_jit(matrix, 10)\n",
    "\n",
    "I get the error:\n",
    "\"This concrete value was not available in Python because it depends on the value of the argument power.\"\n",
    "\n",
    "So instead of using @jax.jit, I call the same function with the following:\n",
    "matrix_power_jit = jax.jit(matrix_power, static_argnums=(1,))\n",
    "power_result_optimized = matrix_power_jit(matrix, power)\n",
    "\n",
    "This is from:\n",
    "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
    "https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html\n",
    "\"\"\"\n",
    "\n",
    "# Optimized matrix power\n",
    "\n",
    "# Define the jit function\n",
    "\n",
    "# matrix_power_jit = jax.jit(matrix_power, static_argnums=(1,)) # static_argnums=(1,) means that the second argument is static.\n",
    "#                                                               # trace- and compile-time constant.\n",
    "#                                                               # https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html\n",
    "\n",
    "matrix_power_jit = jax.jit(matrix_power, static_argnames=('_power',)) # static_argnames=('_power',) means that the '_power' argument is static.\n",
    "                                                                      #                 (the name is the target's function arg name).\n",
    "                                                                      # trace- and compile-time constant.\n",
    "                                                                      # https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html\n",
    "# Call the jit function\n",
    "t2_start = perf_counter()\n",
    "power_result_optimized = matrix_power_jit(matrix, power)\n",
    "t2_stop = perf_counter()\n",
    "\n",
    "print(f\"power_result_optimized: {power_result_optimized}\")\n",
    "\n",
    "print(f\"Unoptimized matrix power time: {t1_stop - t1_start}\")\n",
    "print(f\"Optimized matrix power time: {t2_stop - t2_start}\")\n",
    "\n",
    "\n",
    "# On large power value the complilation of the jit function takes longer than the execution of the unoptimized function.\n",
    "if power > 100:\n",
    "  assert(t2_stop - t2_start > t1_stop - t1_start)\n",
    "  print(f\"Optimization value: {t1_stop - t1_start - (t2_stop - t2_start)}\")\n",
    "else:\n",
    "  assert(t1_stop - t1_start > t2_stop - t2_start)\n",
    "  print(f\"Optimization value: {t1_stop - t1_start - (t2_stop - t2_start)}\")\n",
    "\n",
    "\n",
    "# Call the jit function again\n",
    "t3_start = perf_counter()\n",
    "power_result_optimized_2 = matrix_power_jit(matrix, power)\n",
    "t3_stop = perf_counter()\n",
    "\n",
    "print(f\"power_result_optimized_2: {power_result_optimized_2}\")\n",
    "print(f\"Optimized matrix power time: {t3_stop - t3_start}\")\n",
    "\n",
    "assert(t1_stop - t1_start > t3_stop - t3_start)\n",
    "\n",
    "print(f\"Optimization second time: {t1_stop - t1_start - (t3_stop - t3_start)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_p312_jax",
   "language": "python",
   "name": "venv_p312_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
