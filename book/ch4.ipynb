{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.30\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sigmoid function\n",
    "Sigmoid squashes input to a range between O and 1 , fitting well for binary classification tasks.\n",
    "\n",
    "\"\"\"\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rectified Linear Unit (ReLU)\n",
    "ReLU is a non-linear activation function that outputs the input directly if it is positive, otherwise, it outputs zero.\n",
    "Enhancing effciency compared to sigmoid.\n",
    "\n",
    "\"\"\"\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tanh function\n",
    "Tanh squashes input to a range between -1 and 1, which is useful for classification tasks.\n",
    "(different than Sigmoid that squashes input to a range between O and 1 )\n",
    "\n",
    "\"\"\"\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Softmax function\n",
    "Softmax squashes input to a range between O and 1, and the sum of the output is 1.\n",
    "Used in the output layer for multi-class classification, converting outputs into probabilities.\n",
    "\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Unraveling the Gradient Descent Algorithm.    \n",
    "Backpropagation, the engine behind neural network training, iteratively adjusts weights and biases to minimize the\n",
    "error between predicted and actual outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a simple neural network with one hidden layer\n",
    "def backpropagation(inputs, targets, weights_input_hidden, weights_hidden_output):\n",
    "  # Forward pass\n",
    "  hidden_inputs = np.dot(inputs, weights_input_hidden)\n",
    "  hidden_outputs = sigmoid(hidden_inputs)\n",
    "\n",
    "  final_inputs = np.dot(hidden_outputs, weights_hidden_output)\n",
    "  final_outputs = sigmoid(final_inputs)\n",
    "\n",
    "  # Calculate error\n",
    "  output_errors = targets - final_outputs\n",
    "\n",
    "  # Backward pass\n",
    "  output_grad = final_outputs * (1 - final_outputs) * output_errors\n",
    "  hidden_errors = np.dot(output_grad, weights_hidden_output.T)\n",
    "  hidden_grad = hidden_outputs * (1 - hidden_outputs) * hidden_errors\n",
    "\n",
    "  # Update weights and biases\n",
    "  weights_hidden_output += np.dot(hidden_outputs.T, output_grad)\n",
    "  weights_input_hidden += np.dot(inputs.T, hidden_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize a function by iteratively moving towards the minimum value of the function.     \n",
    "It is particularly useful for training machine learning models.     \n",
    "The primary goal of gradient descent is to identify the model parameters that provide the maximum accuracy on both training and test datasets.     \n",
    "Unlike traditional Gradient Descent, which uses the entire dataset to compute the gradient of the loss function,    \n",
    "SGD randomly selects a subset of data at each step.     \n",
    "This makes SGD faster and more scalable for large datasets, though it may introduce more noise into the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent SGD optimizer\n",
    "def sgd_optimizer(inputs, targets, learning_rate=0.01, epochs=100):\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(len(inputs)):\n",
    "      # Forward pass\n",
    "      # Backward pass and update weights\n",
    "      print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "A training optimization technique used to accelerate the convergence of gradient descent algorithms.    \n",
    "It helps to speed up the learning process by incorporating the direction and velocity of the previous gradients into the current update.    \n",
    "Essentially, it adds a fraction of the previous update step to the current update step,    \n",
    "allowing the algorithm to move faster through shallow regions and dampening oscillations in steep regions.     \n",
    "This technique is inspired by the physical concept of momentum, where an object in motion tends to stay in motion.    \n",
    "In the context of neural network training, it helps to overcome some of the limitations of standard gradient descent    \n",
    "by making the path towards the minimum more direct and thus potentially reducing the number of iterations needed to reach convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_optimizer(inputs, targets, learning_rate=0.01, epochs=100, momentum=0.9):\n",
    "  velocity = 0\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(len(inputs)):\n",
    "      # Forward pass\n",
    "      # Backward pass and update weights\n",
    "      velocity = momentum * velocity + learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Learning Rate\n",
    "A training optimization technique that adjusts the learning rate dynamically during the training of a model.    \n",
    "Unlike fixed learning rate strategies, adaptive methods modify the learning rate for each parameter,    \n",
    "based on the history of gradients for that parameter.     \n",
    "This approach helps in addressing issues like choosing an appropriate learning rate or adjusting it during training,     \n",
    "which can significantly affect the convergence speed and quality of the final model.    \n",
    "Techniques such as AdaGrad, RMSprop, and Adam are examples of adaptive learning rate methods.     \n",
    "They aim to decrease the learning rate for parameters with large gradients to avoid overshooting and   \n",
    "increase it for parameters with small gradients to speed up the learning process.    \n",
    "This results in a more efficient and effective training process, especially for complex models and large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_learning_rate_optimizer(inputs, targets, learning_rate=0.01, epochs=100):\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(len(inputs)):\n",
    "      # Forward pass\n",
    "      # Backward pass and update weights\n",
    "      learning_rate *= 1.0/(1.0 + learning_rate * epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Technique\n",
    "A training optimization technique used to prevent overfitting in machine learning models.     \n",
    "Overfitting occurs when a model learns the training data too well,     \n",
    "capturing noise along with the underlying patterns, which results in poor performance on new, unseen data.     \n",
    "Regularization addresses this issue by adding a penalty on the size of the model parameters to the loss function.     \n",
    "This penalty encourages the model to keep the weights small, which can lead to simpler models that generalize better to new data.     \n",
    "</br>\n",
    "There are several types of regularization techniques, including:     \n",
    "</br>\n",
    "L1 Regularization (Lasso): Adds a penalty equal to the absolute value of the magnitude of coefficients.     \n",
    "This can lead to sparse models where some weights can become zero, effectively performing feature selection.    \n",
    "</br>\n",
    "L2 Regularization (Ridge): Adds a penalty equal to the square of the magnitude of coefficients.     \n",
    "This discourages large weights but does not necessarily drive them to zero.    \n",
    "</br>\n",
    "Elastic Net: Combines L1 and L2 regularization, adding both penalties to the loss function.    \n",
    "This method enjoys the feature selection property of L1 and the smoothing of L2 regularization.   \n",
    "</br>\n",
    "Regularization techniques are widely used in linear regression, logistic regression, neural networks, and     \n",
    "many other machine learning algorithms to improve their generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(inputs, dropout_rate=0.2):\n",
    "  mask = (np.random.rand(*inputs.shape) < 1.0 - dropout_rate) / (1.0 - dropout_rate)\n",
    "  return inputs * mask\n",
    "\n",
    "def weight_decay(weights, decay_rate=0.001):\n",
    "  return weights - decay_rate * weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Multilayer Perceptron (MLP)\n",
    "Implement a simple multilayer perceptron (MLP) for a binary classification problem.    \n",
    "Use NumPy for matrix operations and implement both forward and backward passes.     \n",
    "Additionally, include a training loop to update the weights and biases using gradient descent.  </br>  \n",
    "\n",
    "#### Requirements:\n",
    "1. Design a multilayer perceptron with:\n",
    "   - Input layer with 5 neurons.\n",
    "   - Hidden layer with 10 neurons, using a ReLU activation function.\n",
    "   - Output layer with 1 neuron and a sigmoid activation function.\n",
    "2. Implement forward pass logic to compute the predicted output.\n",
    "3. Implement backward pass logic to calculate gradients and update weights and biases using gradient descent.\n",
    "4. Create a simple dataset for binary classification (e.g., use NumPy to generate random data).\n",
    "5. Train your MLP on the dataset for a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 4.302259694208785\n",
      "Epoch 100, loss 2.6849827636148813\n",
      "Epoch 200, loss 0.29311894603362476\n",
      "Epoch 300, loss 0.20835549379259646\n",
      "Epoch 400, loss 0.1732184134278425\n",
      "Epoch 500, loss 0.15290112611359258\n",
      "Epoch 600, loss 0.13903928116198963\n",
      "Epoch 700, loss 0.12842133841620013\n",
      "Epoch 800, loss 0.11981254043894086\n",
      "Epoch 900, loss 0.11262041788768858\n",
      "Predicted output for data point: [[0.98945626]]\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "import numpy as np\n",
    "\n",
    "# Define the MLP architecture\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "biases_hidden = np.zeros((1, hidden_size))\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "biases_output = np.zeros((1, output_size))\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(inputs):\n",
    "  global weights_hidden_output, biases_output, weights_input_hidden, biases_hidden\n",
    "  hidden_input = np.dot(inputs, weights_input_hidden) + biases_hidden\n",
    "  hidden_output = relu(hidden_input)\n",
    "  final_input = np.dot(hidden_output, weights_hidden_output) + biases_output\n",
    "  predicted_output = sigmoid(final_input)\n",
    "  return predicted_output, hidden_output\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(inputs, predicted_output, hidden_output, targets):\n",
    "  global weights_hidden_output, biases_output, weights_input_hidden, biases_hidden\n",
    "  output_error = predicted_output - targets\n",
    "  output_delta = output_error * (predicted_output * (1 - predicted_output))\n",
    "\n",
    "  hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "  hidden_delta = hidden_error * (hidden_output > 0)\n",
    "\n",
    "  # Update weights and biases\n",
    "  weights_hidden_output -= learning_rate * hidden_output.T.dot(output_delta)\n",
    "  biases_output -= learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "  weights_input_hidden -= learning_rate * inputs.T.dot(hidden_delta)\n",
    "  biases_hidden -= learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, input_size)\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int).reshape(-1, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "  # Forward pass\n",
    "  predicted_output, hidden_output = forward_pass(X)\n",
    "\n",
    "  # Backward pass\n",
    "  backward_pass(X, predicted_output, hidden_output, y)\n",
    "\n",
    "  # Print loss every 100 epochs\n",
    "  if epoch % 100 == 0:\n",
    "    loss = -np.mean(y * np.log(predicted_output) + (1 - y) * np.log(1 - predicted_output))\n",
    "    print(f\"Epoch {epoch}, loss {loss}\")\n",
    "\n",
    "# Test the model\n",
    "data_point = np.array([[0.6, 0.7, 0.8, 0.9, 1.0]])\n",
    "predicted_output, _ = forward_pass(data_point)\n",
    "print(f\"Predicted output for data point: {predicted_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_p312_jax",
   "language": "python",
   "name": "venv_p312_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
