{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.30\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sigmoid function\n",
    "Sigmoid squashes input to a range between O and 1 , fitting well for binary classification tasks.\n",
    "\n",
    "\"\"\"\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rectified Linear Unit (ReLU)\n",
    "ReLU is a non-linear activation function that outputs the input directly if it is positive, otherwise, it outputs zero.\n",
    "Enhancing effciency compared to sigmoid.\n",
    "\n",
    "\"\"\"\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tanh function\n",
    "Tanh squashes input to a range between -1 and 1, which is useful for classification tasks.\n",
    "(different than Sigmoid that squashes input to a range between O and 1 )\n",
    "\n",
    "\"\"\"\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Softmax function\n",
    "Softmax squashes input to a range between O and 1, and the sum of the output is 1.\n",
    "Used in the output layer for multi-class classification, converting outputs into probabilities.\n",
    "\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Unraveling the Gradient Descent Algorithm.    \n",
    "Backpropagation, the engine behind neural network training, iteratively adjusts weights and biases to minimize the\n",
    "error between predicted and actual outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a simple neural network with one hidden layer\n",
    "def backpropagation(inputs, targets, weights_input_hidden, weights_hidden_output):\n",
    "  # Forward pass\n",
    "  hidden_inputs = np.dot(inputs, weights_input_hidden)\n",
    "  hidden_outputs = sigmoid(hidden_inputs)\n",
    "\n",
    "  final_inputs = np.dot(hidden_outputs, weights_hidden_output)\n",
    "  final_outputs = sigmoid(final_inputs)\n",
    "\n",
    "  # Calculate error\n",
    "  output_errors = targets - final_outputs\n",
    "\n",
    "  # Backward pass\n",
    "  output_grad = final_outputs * (1 - final_outputs) * output_errors\n",
    "  hidden_errors = np.dot(output_grad, weights_hidden_output.T)\n",
    "  hidden_grad = hidden_outputs * (1 - hidden_outputs) * hidden_errors\n",
    "\n",
    "  # Update weights and biases\n",
    "  weights_hidden_output += np.dot(hidden_outputs.T, output_grad)\n",
    "  weights_input_hidden += np.dot(inputs.T, hidden_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_p312_jax",
   "language": "python",
   "name": "venv_p312_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
